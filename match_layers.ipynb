{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63200493",
   "metadata": {},
   "source": [
    "## Match Layers\n",
    "\n",
    "This Notebook explore the inner workings of the utility **match_layers.py**\n",
    "\n",
    "used to enable the usage of a Whisper Fine-tuned model inside Whisper codebase.\n",
    "\n",
    "With this NB you can get a **better understanding** of how the utility works\n",
    "\n",
    "(how the matching is done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1562a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# using pickle to serialize the map_dict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae968c",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cba4646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to enable verbose printing of exceptions (+ layers matching name)\n",
    "DEBUG = False\n",
    "\n",
    "# set to True if your custom model has been trained using DDP (multi-gpu)\n",
    "# as in my case, in the custom HF model, keys have a prefix (model.)\n",
    "# it should come from the fact that I have trained on a milti-gpu machine, using DDP\n",
    "DDP_TRAINED = True\n",
    "\n",
    "# if DDP we have to add a prefix to match with the HF state_dict\n",
    "if DDP_TRAINED:\n",
    "    PREFIX = \"model.\"\n",
    "else:\n",
    "    PREFIX = \"\"\n",
    "    \n",
    "# for now, tested only with medium\n",
    "MODEL_SIZE = \"medium\"\n",
    "\n",
    "# the device where you're running this code\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# the name of the file with your fine-tuned model\n",
    "FINE_TUNED_MODEL = \"medium-custom.pt\"\n",
    "\n",
    "\n",
    "# the name of the file for the serialized map_dict\n",
    "# a different name, to avoid overwrite it \n",
    "FILE_DICT = \"map_dict_test.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36daa8",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6add8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_numbers(inputString):\n",
    "    return any(char.isdigit() for char in inputString)\n",
    "\n",
    "# next functions are used to make sanity checks for the mappings\n",
    "\n",
    "# get if it is encoder or decoder\n",
    "def extract_function(key_name):\n",
    "    # encoder or decoder is the first part of the key\n",
    "    first_part = key_name.split(\".\")[0]\n",
    "\n",
    "    key_func = None\n",
    "    if first_part in [\"enconder\", \"decoder\"]:\n",
    "        key_func = first_part\n",
    "\n",
    "    return key_func\n",
    "\n",
    "def extract_layer_num(key_name):\n",
    "    # layer num is the third piece\n",
    "    layer_num = None\n",
    "\n",
    "    if has_numbers(key_name):\n",
    "        layer_num = key_name.split(\".\")[2]\n",
    "\n",
    "    return layer_num\n",
    "\n",
    "# check that the two keys are for layers \n",
    "# with the same function\n",
    "# (both encoder or both decoder)\n",
    "# and have the same layer number\n",
    "# this way we are super-safe (I think)\n",
    "def sanity_check(key1, key2):\n",
    "    is_ok = True\n",
    "\n",
    "    # check same func (encoder or decoder)\n",
    "    func1 = extract_function(key1)\n",
    "    func2 = extract_function(key2)\n",
    "\n",
    "    if func1 != func2:\n",
    "        print(f\"Warning: layers seem to have different functions: {key1},{key2}\")\n",
    "        is_ok = False\n",
    "\n",
    "    # check same layer_num\n",
    "    layer1 = extract_layer_num(key1)\n",
    "    layer2 = extract_layer_num(key2)\n",
    "\n",
    "    if layer1 != layer2:\n",
    "        print(f\"Warning: layers seem to have different numbers: {key1},{key2}\")\n",
    "        is_ok = False\n",
    "\n",
    "    return is_ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac35bf",
   "metadata": {},
   "source": [
    "### Loading vanilla models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7d1481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading vanilla Whisper model\n",
      "Loading vanilla HF Model\n"
     ]
    }
   ],
   "source": [
    "# Vanilla means: not custom trained\n",
    "print()\n",
    "print(\"Loading vanilla Whisper model\")\n",
    "model = whisper.load_model(MODEL_SIZE, device=DEVICE)\n",
    "\n",
    "print(\"Loading vanilla HF Model\")\n",
    "hugging_face_model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    \"openai/whisper-\" + MODEL_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2138ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract state-dict from both\n",
    "state_d_openai = model.state_dict()\n",
    "state_d_huggingface = hugging_face_model.model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853e793",
   "metadata": {},
   "source": [
    "### do the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1284eb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [00:15<00:00, 60.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# build the mapping between keys...\n",
    "map_dict = {}\n",
    "print(\"Matching layers...\")\n",
    "\n",
    "# for every layer in OpenAI model\n",
    "n_sanity_ok = 0\n",
    "\n",
    "#\n",
    "# here we're considering the cartesian product of the two state dict and try to match\n",
    "# rules applied: \n",
    "# 1. the two layers have the same shape\n",
    "# 2. the two layer have the same parameters' values\n",
    "# 3. we apply sanity check (see function above)\n",
    "#\n",
    "for k in tqdm(state_d_openai):\n",
    "    # find a layer in the HF model, check with j\n",
    "    for j in state_d_huggingface:\n",
    "        # where parameters have same shape and same values\n",
    "        if state_d_huggingface[j].shape == state_d_openai[k].shape:\n",
    "            if torch.all(torch.eq(state_d_huggingface[j], state_d_openai[k])).item():\n",
    "                # found, register the mapping\n",
    "                map_dict[k] = j\n",
    "                # make some check and eventually print a warning\n",
    "                if sanity_check(k, j) == True:\n",
    "                    n_sanity_ok += 1\n",
    "                    \n",
    "                    # if you enable thsi print you can see the name of the layer\n",
    "                    # chosen in the match and you will se that they have the same functions\n",
    "                    if DEBUG:\n",
    "                        print(k, j)\n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e81420",
   "metadata": {},
   "source": [
    "### Additional checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e42299a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if we have matched every entry in state_dict...\n",
      "\n",
      "Number of keys: 947\n",
      "Number of sanity_check ok: 947\n",
      "\n",
      "Match is complete !!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if we have matched every entry\n",
    "print(\"Check if we have matched every entry in state_dict...\")\n",
    "print()\n",
    "print(f\"Number of keys: {len(map_dict.keys())}\")\n",
    "assert len(map_dict.keys()) == len(state_d_openai.keys()), \"The match is not complete !\"\n",
    "\n",
    "print(f\"Number of sanity_check ok: {n_sanity_ok}\")\n",
    "print()\n",
    "\n",
    "print(\"Match is complete !!!\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf11399",
   "metadata": {},
   "source": [
    "### Serialize map dict to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af580f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing map_dict...\n",
      "map_dict saved as: map_dict_test.pkl...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# serialize the map_dict to file\n",
    "print(\"Serializing map_dict...\")\n",
    "\n",
    "with open(FILE_DICT, \"wb\") as f:\n",
    "    pickle.dump(map_dict, f)\n",
    "    f.close()\n",
    "\n",
    "print(f\"map_dict saved as: {FILE_DICT}...\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7d8b6",
   "metadata": {},
   "source": [
    "### Test to see if a custom model can be actually loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f6e77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading map_dict...\n",
      "\n",
      "Loading fine tuned dict...\n",
      "Rebuild the state dict...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 947/947 [00:00<00:00, 1760640.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# loading with match keys\n",
    "# restart from pickle file\n",
    "print(\"Reloading map_dict...\")\n",
    "print()\n",
    "with open(FILE_DICT, \"rb\") as f:\n",
    "    map_dict = pickle.load(f)\n",
    "\n",
    "# loading fine-tuned dict\n",
    "print(\"Loading fine tuned dict...\")\n",
    "# added map_location to handle the fact that the custom model has been trained on GPU\n",
    "state_dict_finetuned = torch.load(FINE_TUNED_MODEL, map_location=torch.device(DEVICE))\n",
    "\n",
    "# build the state_dict to be used\n",
    "# take the key name from standard (OpenAI) and the value from finetuned (HF)\n",
    "print(\"Rebuild the state dict...\")\n",
    "new_state_dict = {}\n",
    "n_except = 0\n",
    "for k in tqdm(map_dict.keys()):\n",
    "    try:\n",
    "        # You must add \"model.\" if you have used DDP in custom training\n",
    "        # see DDP_TRAINED above\n",
    "        # PREFIX is added to a HF fine-tuned 8with DDP). It is not in vanulla HF models\n",
    "        new_state_dict[k] = state_dict_finetuned[PREFIX + map_dict[k]]\n",
    "    except:\n",
    "        n_except += 1\n",
    "\n",
    "        if DEBUG:\n",
    "            print(PREFIX + map_dict[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95db76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_err = f\"Rebuild state dict failed, {n_except} pick failed\"\n",
    "assert n_except == 0, msg_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02d93b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading the final model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print()\n",
    "print(\"Loading the final model...\")\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a38b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
